{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dsci511_header.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Advanced Wrangling with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instructions\n",
    "\n",
    "rubric={mechanics:4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "plt.rcParams.update({'font.size': 14, 'axes.labelweight': 'bold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "rubric={autograde:4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6cd0b7f72a4c69db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In Lab 3, you performed some basic Pandas operations on the Gapminder dataset. However, this dataset was given to you clean and shiny and ready-to-go. In the real world, that's rarely the case, and in this exercise you'll have to clean up a \"dirty\" version of the Gapminder dataset.\n",
    "\n",
    "Your goal is to load in \"dirty Gapminder\" as a dataframe called `dirty` and \"clean Gapminder\" as a dataframe called `clean`, and wrangle `dirty` until it is the same as `clean`:\n",
    "- Dirty Gapminder: <https://raw.githubusercontent.com/STAT545-UBC/STAT545-UBC.github.io/master/gapminderDataFiveYear_dirty.txt>\n",
    "- Clean Gapminder: <https://raw.githubusercontent.com/STAT545-UBC/STAT545-UBC.github.io/master/gapminderDataFiveYear.txt>\n",
    "\n",
    "A test has been provided to check that `dirty` is the same as `clean`. Things you might want to do to clean up `dirty`:\n",
    "\n",
    "- Check that `dirty` and `clean` have the same columns;\n",
    "- Check if there is any missing data, if there is missing data (NaNs or empty strings) fill them with sensible values;\n",
    "- Check for things like capitalization, spelling, etc;\n",
    "- There may be entries that appear to have the exact same spelling and capitalization in both `dirty` and `clean`, but still don't match... Extra whitespace is often a frustrating (and invisible) problem when wrangling text data. You can use `Series.str.strip()` to trim any additional unwanted whitespace around a string.\n",
    "- At any time, you can check which rows in `dirty` are not equal to `clean` using something like: `dirty[dirty.ne(clean).any(axis=1)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-934624b6ddd42195",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "rubric={autograde:4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you are hired as a data scientist for the [Narrabeen](http://narrabeen.wrl.unsw.edu.au/background/#overview) beach survey program in Sydney, Australia. The survey program started in the 1970's and has continued to the present day. All the data is available at: <http://narrabeen.wrl.unsw.edu.au/explore_data/time_series/>.\n",
    "\n",
    "In the next few exercises we're going to download, wrangle, and explore this time-series dataset.\n",
    "\n",
    "<img src=\"img/narrabeen.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survey program is aimed to measure the width of the beach every few weeks. There are five locations along the beach for which measurements are made, from location 1 at the northern end of the beach, to location 5 at the southern end:\n",
    "\n",
    "![](img/survey_locations.jpg)\n",
    "\n",
    "A function `get_beach_data()` is provided for you in the `scraper.py` module that \"scrapes\" the <http://narrabeen.wrl.unsw.edu.au> website for data using the `requests` library (you'll learn more about web scraping in DSCI 525).\n",
    "This function returns a list of tuples containing the date of a survey and the width of the beach on that date, for the given `survey_location` (i.e., 1, 2, 3, 4, or 5). For example:\n",
    "\n",
    "```python\n",
    ">>>get_beach_data(survey_location=1)\n",
    "[('1976-04-27', '78.5'),\n",
    " ('1976-05-10', '65.1'),\n",
    " ('1976-05-18', '72.9'),\n",
    " ('1976-05-25', '76.0'),\n",
    " ('1976-06-02', '83.4'),\n",
    " ('1976-06-16', '67.7'),\n",
    " ('1976-06-18', '74.5'),\n",
    " ('1976-06-23', '78.2'),\n",
    " .\n",
    " .\n",
    " .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Tasks:**\n",
    "\n",
    "1. Import the `get_beach_data()` function from the `scraper.py` module.\n",
    "\n",
    "1. Extract the data for each survey location (1, 2, 3, 4, 5) and store them in five separate `Series` with the Index of the `Series` being the datetime. You'll need to convert the string dates to datetimes with `pd.to_datetime()`.\n",
    "\n",
    "1. Merge all of the `Series` together in a single dataframe. Note that the `Series` data are not all the same length (some measurements are missing), so we want to do an \"inner\" join on the index values between all the `Series` here.\n",
    "\n",
    "Your final dataframe should be called `beach_df` and should look like this (note that the datetimes have been set as the index):\n",
    "\n",
    "```\n",
    "            location_1  location_2  location_3  location_4  location_5\n",
    "1976-04-27        78.5        59.7        44.7        16.2        63.5\n",
    "1976-05-10        65.1        60.7        61.6        15.0        65.8\n",
    "1976-05-18        72.9        67.0        68.6        23.8        64.3\n",
    "1976-05-25        76.0        70.3        67.6        21.8        44.5\n",
    "1976-06-02        83.4        79.5        79.8        24.5        57.1\n",
    "...                ...         ...         ...         ...         ...\n",
    "```\n",
    "\n",
    "The next few questions of this lab rely on this dataframe, so make sure that you pass the autograder tests before proceeding.\n",
    "\n",
    "If you are unable to pass the tests, I've saved a version of the dataframe in the data folder of this directory. When you get to Exercise 3, you may just read that in with\n",
    "\n",
    "```python\n",
    "pd.read_csv('data/beach_data.csv', index_col=0, parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "rubric={accuracy:4,efficiency:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 2, you may have noticed that the frequency of the data you just collected is on the order of a few days to a few weeksâ€”it's irregularly spaced.\n",
    "This can make it difficult to extract patterns from the data. One way to deal with this is to resample the data to regular intervals (e.g. from irregular daily data to monthly data).\n",
    "\n",
    "**Note:** If you were unable to wrangle the data and pass the autograder tests in Exercise 2, you can find a version of the `beach_df` in the `data` folder which you can read in using:\n",
    "\n",
    "```python\n",
    "pd.read_csv('data/beach_data.csv', index_col=0, parse_dates=True)\n",
    "```\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "1. First, subtract the mean value of each column from the same column. This will help to see if a location on the beach is narrower (negative numbers) or wider (positive numbers) at a certain time compared to the average.\n",
    "\n",
    "1. Resample `beach_df` from the previous question to monthly intervals with the mean as the aggregation function. Use `.resample()` in a way that it converts the index type of the dataframe to `PeriodIndex`.\n",
    "\n",
    "3. Remove all entries prior to 1980. We do this because the accuracy of these measurements is not great.\n",
    "**Hint:** Remember that you can use datetime indexing in Pandas with [partial string matching](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#partial-string-indexing).\n",
    "\n",
    "1. Finally, melt the dataframe such that your final `beach_df` looks similar to this, and show it in the output:\n",
    "\n",
    "```\n",
    "            location      width\n",
    "datetime                       \n",
    "1980-01   location_1   4.356982\n",
    "1980-02   location_1   3.856982\n",
    "1980-03   location_1   0.156982\n",
    "1980-04   location_1   4.856982\n",
    "1980-05   location_1 -14.343018\n",
    "...              ...        ...\n",
    "2019-07   location_5  -7.042108\n",
    "2019-08   location_5  -6.225441\n",
    "2019-09   location_5 -14.425441\n",
    "2019-10   location_5 -15.125441\n",
    "2019-11   location_5 -15.542108\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you should use the `beach_df` dataframe resulting from your work in Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chaining various Pandas methods together and showing the output, answer the following question:\n",
    "\n",
    "Which survey location has the greatest variability in measured beach widths? Measure variability in terms of standard deviation.\n",
    "\n",
    "**Note:** Don't overwrite `beach_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.2\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataset in an appropriate way, and show the output to answer the following question:\n",
    "\n",
    "Which survey location experienced the greatest erosion (i.e. the largest decrease in width) in the entire dataset?\n",
    "\n",
    "**Note:** Don't overwrite `beach_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.3\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that shows in the output the top three months during which the beach is narrowest based on the average value for width.\n",
    "\n",
    "You can leave the output months as numbers inside a dataframe along with the corresponding `width`s.\n",
    "\n",
    "**Hint:** Decompose the datetime index to extract the month, and use it to create a new `month` column in the original `beach_df` dataframe. After this, you're going to be able to use `.groupby()` to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.4\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the correlation between beach width observations at location 1 and location 5.\n",
    "\n",
    "> **Read if you're interested to know the significance of this computation**: One interesting thing about embayed beaches like Narrabeen is that the amount of sand in the system is typically conserved. That means that if one part of the beach is getting narrower, another section is probably getting wider. This is most obvious at the extremes of the beach, location 1 and location 5 in our case. We can confirm this by calculating the correlation between observations of these two locations.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- The correlation value should be negative, which means that if one location is eroding, the other location is doing the opposite.\n",
    "- There is a dataframe method that calculates correlation between columns which you can use.\n",
    "- Don't overwrite `beach_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.5\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a double `.groupby()` to determine the **month** AND **location** for which the beach is the widest.\n",
    "\n",
    "**Note:** Whenever you do a `.groupby()` operation, you need an aggregation function to summarize information in multiple rows in just one row (we'll learn more about these kinds of operations in DSCI 513).\n",
    "Here, use the median aggregator function to summarize information in rows with similar month and location values, and then try to find what month-location pair that corresponds to the maximum measurement for beach width.\n",
    "\n",
    "> **Read if you're interested to know the significance of this computation**: In the previous questions we learned that:\n",
    ">\n",
    ">- The northern-most beach location (location 1) is the most exposed to waves,\n",
    ">- The biggest waves occur in Australia's winter, but that if one part of the beach is becoming narrower, then the opposite end is probably getting wider.\n",
    ">\n",
    ">Therefore we'd expect the answer here to be location 5 (opposite end of the beach to location 1) some time around the middle of the year (winter-ish time in Australia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.6\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For how many months in our monthly resampled date spanning 1980-2019 do we have at least one missing observation for a location?\n",
    "\n",
    "**Hint:** Use the `.any()` method of Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 4, we learned that there is a \"rotation effect\" at Narrabeen Beach where if one end of the beach is getting narrower, often the other end is getting wider. This is most obvious at the extremes of the beach; that is, location 1 and location 5.\n",
    "\n",
    "Your task here is to show a visual confirmation of the rotation phenomenon by plotting the observations for location 1 and location 5 on the same figure. Remember that Pandas has a built-in plotting method for its dataframes.\n",
    "\n",
    "- Resample the data to your desired periods. The choice of resampling period should be made such that it helps you better see the patterns.\n",
    "\n",
    "- Drop `NaN` values before plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dsci511')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> assert dirty.equals(clean)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> assert all(beach_df.columns == ['location_1', 'location_2', 'location_3', 'location_4', 'location_5'])\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> assert beach_df.shape == (623, 5)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> assert beach_df.iloc[0, 0] == 78.5\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> assert beach_df.iloc[-1, -1] == 39.6\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "e48f8b1687318edbd5a2a918b592db3baee1b5f69ffdc30179f0c7d8337e101b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
